# DocSmart RAG System - Proyecto Final

**AWS AI Engineer Nanodegree Program**  
**Udacity + Amazon Web Services**

![AWS](https://img.shields.io/badge/AWS-Bedrock-orange)
![Terraform](https://img.shields.io/badge/IaC-Terraform-blue)
![Python](https://img.shields.io/badge/Python-3.12-green)
![Aurora](https://img.shields.io/badge/Database-Aurora_PostgreSQL-blue)

---

## üìã Tabla de Contenidos

- [Descripci√≥n General](#-descripci√≥n-general)
- [Arquitectura del Sistema](#-arquitectura-del-sistema)
- [Requisitos Previos](#-requisitos-previos)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Instalaci√≥n y Configuraci√≥n](#-instalaci√≥n-y-configuraci√≥n)
  - [Paso 1: Clonar el Repositorio](#paso-1-clonar-el-repositorio)
  - [Paso 2: Desplegar Stack 1 (Infraestructura Base)](#paso-2-desplegar-stack-1-infraestructura-base)
  - [Paso 3: Inicializar Base de Datos](#paso-3-inicializar-base-de-datos)
  - [Paso 4: Desplegar Stack 2 (Knowledge Base)](#paso-4-desplegar-stack-2-knowledge-base)
  - [Paso 5: Cargar Documentos](#paso-5-cargar-documentos)
  - [Paso 6: Sincronizar Knowledge Base](#paso-6-sincronizar-knowledge-base)
- [Uso del Sistema](#-uso-del-sistema)
- [Funciones Implementadas](#-funciones-implementadas)
- [Par√°metros del Modelo](#-par√°metros-del-modelo)
- [Pruebas y Validaci√≥n](#-pruebas-y-validaci√≥n)
- [Capturas de Pantalla](#-capturas-de-pantalla)
- [Soluci√≥n de Problemas](#-soluci√≥n-de-problemas)
- [Contribuciones](#-contribuciones)
- [Licencia](#-licencia)

---

## üéØ Descripci√≥n General

**DocSmart RAG System** es un sistema de Retrieval-Augmented Generation (RAG) construido sobre Amazon Bedrock que permite consultar pol√≠ticas de recursos humanos de manera conversacional e inteligente.

### Caracter√≠sticas Principales

- ‚úÖ **Infraestructura como C√≥digo (IaC)** con Terraform
- ‚úÖ **Base de Datos Vectorial** con Aurora Serverless PostgreSQL + pgvector
- ‚úÖ **Knowledge Base de Bedrock** para almacenamiento de documentos
- ‚úÖ **B√∫squeda H√≠brida** (sem√°ntica + palabras clave)
- ‚úÖ **Interfaz Web** con Streamlit (tema oscuro profesional)
- ‚úÖ **Procesamiento de Lenguaje Natural** con Claude 3.5 Sonnet
- ‚úÖ **Validaci√≥n de Prompts** para seguridad y categorizaci√≥n
- ‚úÖ **Soporte Multiling√ºe** (espa√±ol e ingl√©s)

### Tecnolog√≠as Utilizadas

- **AWS Bedrock**: Claude 3.5 Sonnet, Titan Embeddings v2
- **AWS Aurora Serverless v2**: PostgreSQL 15.5 con pgvector
- **AWS S3**: Almacenamiento de documentos
- **Terraform**: Gesti√≥n de infraestructura
- **Python 3.12**: L√≥gica de aplicaci√≥n
- **Streamlit**: Interfaz de usuario web
- **boto3**: SDK de AWS para Python

---

## üèóÔ∏è Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Usuario Final                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Streamlit Web Interface                       ‚îÇ
‚îÇ                      (app_demo.py)                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     bedrock_utils.py                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇvalid_prompt()‚îÇ  ‚îÇquery_kb()    ‚îÇ  ‚îÇgenerate()    ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                  ‚îÇ                 ‚îÇ
          ‚ñº                  ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AWS Bedrock    ‚îÇ ‚îÇ  Bedrock        ‚îÇ ‚îÇ  Bedrock         ‚îÇ
‚îÇ  Agent Runtime  ‚îÇ ‚îÇ  Knowledge Base ‚îÇ ‚îÇ  Runtime         ‚îÇ
‚îÇ  (Validation)   ‚îÇ ‚îÇ  (Retrieval)    ‚îÇ ‚îÇ  (Generation)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ    Aurora PostgreSQL         ‚îÇ
              ‚îÇ    + pgvector extension      ‚îÇ
              ‚îÇ                              ‚îÇ
              ‚îÇ  bedrock_integration schema  ‚îÇ
              ‚îÇ  bedrock_kb table            ‚îÇ
              ‚îÇ  1024-dim vectors            ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚ñ≤
                             ‚îÇ
                             ‚îÇ (Ingestion)
                             ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ         S3 Bucket            ‚îÇ
              ‚îÇ    (Document Storage)        ‚îÇ
              ‚îÇ                              ‚îÇ
              ‚îÇ  spec-sheets/                ‚îÇ
              ‚îÇ  politica_vacaciones.pdf     ‚îÇ
              ‚îÇ  manual_empleado.docx        ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Flujo de Datos

1. **Usuario** hace una pregunta en la interfaz Streamlit
2. **valid_prompt()** valida y categoriza la consulta
3. **query_knowledge_base()** busca documentos relevantes en Aurora
4. **Bedrock Knowledge Base** retorna chunks con embeddings similares
5. **generate_response()** genera respuesta usando Claude + contexto
6. **Respuesta** se muestra en la interfaz con fuentes citadas

---

## üì¶ Requisitos Previos

### Software Requerido

- **Terraform**: >= 1.0
  ```bash
  terraform --version
  ```

- **AWS CLI**: >= 2.0
  ```bash
  aws --version
  ```

- **Python**: >= 3.11
  ```bash
  python --version
  ```

- **Git**: Para clonar el repositorio
  ```bash
  git --version
  ```

### Credenciales AWS

- Cuenta de AWS con permisos para:
  - VPC, EC2 (Subnets, Security Groups)
  - RDS (Aurora Serverless)
  - S3 (Buckets, Objetos)
  - Bedrock (Knowledge Base, Models)
  - IAM (Roles, Policies)
  - Secrets Manager
  - CloudWatch Logs

- Credenciales configuradas:
  ```bash
  aws configure
  # O usar AWS Academy Learner Lab credentials
  ```

### Modelos de Bedrock Habilitados

Habilitar en AWS Console > Bedrock > Model Access:
- ‚úÖ **Claude 3.5 Sonnet v1** (anthropic.claude-3-5-sonnet-20240620-v1:0)
- ‚úÖ **Titan Embeddings Text v2** (amazon.titan-embed-text-v2:0)

---

## üìÅ Estructura del Proyecto

```
docsmart-rag-system/
‚îÇ
‚îú‚îÄ‚îÄ stack1/                          # Infraestructura base
‚îÇ   ‚îú‚îÄ‚îÄ main.tf                      # VPC, Aurora, S3, IAM
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf                 # Variables de configuraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf                   # Outputs para Stack 2
‚îÇ   ‚îî‚îÄ‚îÄ terraform.tfvars.example     # Ejemplo de configuraci√≥n
‚îÇ
‚îú‚îÄ‚îÄ stack2/                          # Bedrock Knowledge Base
‚îÇ   ‚îú‚îÄ‚îÄ main.tf                      # KB, Data Source, Secrets
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf                 # Variables de KB
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf                   # IDs y configuraci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ terraform.tfvars.example     # Ejemplo de configuraci√≥n
‚îÇ
‚îú‚îÄ‚îÄ scripts/                         # Scripts de utilidad
‚îÇ   ‚îú‚îÄ‚îÄ aurora_init.sql              # Inicializaci√≥n de DB
‚îÇ   ‚îú‚îÄ‚îÄ aurora_verify.sql            # Verificaci√≥n de setup
‚îÇ   ‚îî‚îÄ‚îÄ upload_to_s3.py              # Carga de documentos
‚îÇ
‚îú‚îÄ‚îÄ spec-sheets/                     # Documentos a indexar
‚îÇ   ‚îî‚îÄ‚îÄ (tus archivos PDF, DOCX, TXT)
‚îÇ
‚îú‚îÄ‚îÄ screenshots/                     # Capturas para entrega
‚îÇ   ‚îî‚îÄ‚îÄ (capturas de pantalla aqu√≠)
‚îÇ
‚îú‚îÄ‚îÄ bedrock_utils.py                 # Funciones principales
‚îú‚îÄ‚îÄ app_demo.py                      # Interfaz Streamlit
‚îú‚îÄ‚îÄ config.py                        # Configuraci√≥n global
‚îú‚îÄ‚îÄ rag_system.py                    # Sistema RAG (alternativo)
‚îú‚îÄ‚îÄ requirements.txt                 # Dependencias Python
‚îÇ
‚îú‚îÄ‚îÄ temperature_top_p_explanation.md # Documentaci√≥n de par√°metros
‚îú‚îÄ‚îÄ README_FINAL_PROJECT.md          # Este archivo
‚îî‚îÄ‚îÄ SCREENSHOT_GUIDE.md              # Gu√≠a de capturas
```

---

## üöÄ Instalaci√≥n y Configuraci√≥n

### Paso 1: Clonar el Repositorio

```bash
# Clonar repositorio
git clone https://github.com/tu-usuario/docsmart-rag-system.git
cd docsmart-rag-system

# Instalar dependencias Python
pip install -r requirements.txt
```

### Paso 2: Desplegar Stack 1 (Infraestructura Base)

**Stack 1** crea: VPC, Aurora Serverless PostgreSQL, S3 Bucket, IAM Roles

```bash
# Navegar a stack1
cd stack1

# Copiar y editar variables
cp terraform.tfvars.example terraform.tfvars
nano terraform.tfvars

# Editar estos valores:
# - s3_bucket_name: Debe ser √∫nico globalmente
# - database_master_password: Contrase√±a segura (min 8 caracteres)
# - aws_region: Tu regi√≥n preferida (ej: us-east-1)

# Inicializar Terraform
terraform init

# Revisar plan de despliegue
terraform plan

# Desplegar infraestructura
terraform apply

# Escribir "yes" para confirmar
```

**Tiempo estimado:** 10-15 minutos

**Outputs importantes:**
```bash
# Guardar estos valores para Stack 2
aurora_cluster_endpoint = "docsmart-aurora-cluster.cluster-xxxxx.us-east-1.rds.amazonaws.com"
aurora_cluster_arn = "arn:aws:rds:us-east-1:123456789012:cluster:docsmart-aurora-cluster"
s3_bucket_name = "docsmart-documents-967663481769"
bedrock_kb_role_arn = "arn:aws:iam::123456789012:role/docsmart-bedrock-kb-role"
database_name = "docsmart_kb"
```

### Paso 3: Inicializar Base de Datos

Ejecutar scripts SQL usando **RDS Query Editor** o **psql**:

#### Opci√≥n A: AWS Console (Query Editor)

1. Ir a **AWS Console > RDS > Query Editor**
2. Seleccionar cluster `docsmart-aurora-cluster`
3. Autenticarse con credenciales de Stack 1
4. Abrir y ejecutar `scripts/aurora_init.sql`
5. Verificar con `scripts/aurora_verify.sql`

#### Opci√≥n B: psql (CLI)

```bash
# Obtener endpoint de Stack 1 output
export DB_ENDPOINT="docsmart-aurora-cluster.cluster-xxxxx.us-east-1.rds.amazonaws.com"
export DB_NAME="docsmart_kb"
export DB_USER="dbadmin"
export DB_PASSWORD="YourSecurePassword123!"

# Conectar y ejecutar
psql -h $DB_ENDPOINT -U $DB_USER -d $DB_NAME -f scripts/aurora_init.sql

# Verificar
psql -h $DB_ENDPOINT -U $DB_USER -d $DB_NAME -f scripts/aurora_verify.sql
```

**Verificaci√≥n exitosa:**
```
‚úì pgvector is installed
‚úì Schema exists
‚úì bedrock_kb table
‚úì Required indexes (4)
‚úì Search function
‚úì Monitoring views
‚úì‚úì‚úì DATABASE IS READY FOR BEDROCK KNOWLEDGE BASE ‚úì‚úì‚úì
```

### Paso 4: Desplegar Stack 2 (Knowledge Base)

**Stack 2** crea: Bedrock Knowledge Base, Data Source (S3), Secrets Manager

```bash
# Navegar a stack2
cd ../stack2

# Copiar y editar variables
cp terraform.tfvars.example terraform.tfvars
nano terraform.tfvars

# IMPORTANTE: Usar los outputs de Stack 1
# - s3_bucket_name: De Stack 1 output
# - aurora_cluster_arn: De Stack 1 output
# - database_name: De Stack 1 output
# - database_master_username: Mismo que Stack 1
# - database_master_password: Mismo que Stack 1
# - bedrock_kb_role_name: De Stack 1 output

# Inicializar Terraform
terraform init

# Revisar plan
terraform plan

# Desplegar Knowledge Base
terraform apply

# Escribir "yes" para confirmar
```

**Tiempo estimado:** 5-10 minutos

**Outputs importantes:**
```bash
knowledge_base_id = "ABCDEFGHIJ"
data_source_id = "KLMNOPQRST"
sync_data_source_command = "aws bedrock-agent start-ingestion-job ..."
```

### Paso 5: Cargar Documentos

Subir documentos HR a S3 para indexaci√≥n:

```bash
# Volver al directorio ra√≠z
cd ..

# Agregar tus documentos a spec-sheets/
cp /path/to/your/politica_vacaciones.pdf spec-sheets/
cp /path/to/your/manual_empleado.docx spec-sheets/

# Editar script de carga
nano scripts/upload_to_s3.py
# Actualizar BUCKET_NAME con tu bucket de Stack 1

# Ejecutar script de carga
python scripts/upload_to_s3.py
```

**Output esperado:**
```
======================================================================
Starting upload from 'spec-sheets' to s3://docsmart-documents-967663481769/
======================================================================

Found 3 file(s) to upload.

  Uploading politica_vacaciones.pdf (0.25 MB)... ‚úì
  Uploading manual_empleado.docx (0.50 MB)... ‚úì
  Uploading beneficios_empresa.txt (0.05 MB)... ‚úì

======================================================================
Upload Summary:
  ‚úì Successful: 3
  Total files: 3
======================================================================
```

### Paso 6: Sincronizar Knowledge Base

Indexar documentos en Bedrock Knowledge Base:

#### Opci√≥n A: AWS CLI (Recomendado)

```bash
# Usar comando de Stack 2 output
aws bedrock-agent start-ingestion-job \
  --knowledge-base-id ABCDEFGHIJ \
  --data-source-id KLMNOPQRST \
  --region us-east-1

# Monitorear progreso
aws bedrock-agent list-ingestion-jobs \
  --knowledge-base-id ABCDEFGHIJ \
  --data-source-id KLMNOPQRST \
  --region us-east-1
```

#### Opci√≥n B: AWS Console

1. Ir a **AWS Console > Bedrock > Knowledge Bases**
2. Seleccionar `docsmart-knowledge-base`
3. Ir a **Data Sources**
4. Clic en `docsmart-s3-data-source`
5. Clic en **Sync**
6. Esperar a que el estado sea `COMPLETE`

**Tiempo de sincronizaci√≥n:** 2-5 minutos (depende del tama√±o de documentos)

**Verificaci√≥n:**
```bash
# Estado del job debe ser "COMPLETE"
{
  "ingestionJobId": "xxxxx",
  "status": "COMPLETE",
  "statistics": {
    "numberOfDocumentsScanned": 3,
    "numberOfNewDocumentsIndexed": 3,
    "numberOfModifiedDocumentsIndexed": 0,
    "numberOfDocumentsDeleted": 0,
    "numberOfDocumentsFailed": 0
  }
}
```

---

## üéÆ Uso del Sistema

### Configurar Variables de Entorno

```bash
# Crear archivo .env en el directorio ra√≠z
cat > .env << EOF
AWS_REGION=us-east-1
KNOWLEDGE_BASE_ID=ABCDEFGHIJ
EMBEDDING_MODEL_ID=amazon.titan-embed-text-v2:0
LLM_MODEL_ID=anthropic.claude-3-5-sonnet-20240620-v1:0
EOF
```

### Lanzar Interfaz Web

```bash
# Ejecutar Streamlit
python -m streamlit run app_demo.py --server.port 8501

# Abrir navegador en:
# http://localhost:8501
```

### Probar Consultas

Ejemplos de preguntas:

```
‚úÖ "¬øCu√°ntos d√≠as de vacaciones tengo?"
‚úÖ "¬øA m√≠ cu√°nto me toca? Estoy hace 1 a√±o"
‚úÖ "¬øC√≥mo solicito vacaciones?"
‚úÖ "¬øQu√© beneficios tiene la empresa?"
‚úÖ "¬øCu√°l es el proceso de renovaci√≥n de contrato?"
```

### Usar desde Python

```python
from bedrock_utils import rag_pipeline

# Ejecutar pipeline completo
result = rag_pipeline(
    user_query="¬øCu√°ntos d√≠as de vacaciones tengo?",
    knowledge_base_id="ABCDEFGHIJ",
    temperature=0.3,
    top_p=0.9
)

print(result['final_response'])
print(f"\nFuentes consultadas: {len(result['retrieval']['results'])}")
```

---

## ‚öôÔ∏è Funciones Implementadas

### 1. `query_knowledge_base()`

**Ubicaci√≥n:** `bedrock_utils.py`

Consulta Bedrock Knowledge Base para recuperar documentos relevantes.

```python
def query_knowledge_base(
    query: str,
    knowledge_base_id: str = KNOWLEDGE_BASE_ID,
    max_results: int = 5,
    score_threshold: float = 0.1
) -> Dict[str, Any]:
    """
    Retrieves relevant documents from Bedrock Knowledge Base.
    
    Returns:
        {
            'results': List[Dict],  # Retrieved documents
            'count': int,           # Number of results
            'query': str            # Original query
        }
    """
```

**Caracter√≠sticas:**
- ‚úÖ B√∫squeda h√≠brida (sem√°ntica + keywords)
- ‚úÖ Filtrado por score threshold
- ‚úÖ Metadata de documentos incluida
- ‚úÖ Manejo de errores AWS

### 2. `generate_response()`

**Ubicaci√≥n:** `bedrock_utils.py`

Genera respuesta usando Claude 3.5 Sonnet con contexto recuperado.

```python
def generate_response(
    query: str,
    context_documents: List[Dict[str, Any]],
    model_id: str = LLM_MODEL_ID,
    temperature: float = 0.7,
    top_p: float = 0.9,
    max_tokens: int = 1000
) -> Dict[str, Any]:
    """
    Generates response using Bedrock LLM with retrieved context.
    
    Returns:
        {
            'response': str,         # Generated text
            'model_id': str,         # Model used
            'sources': List[Dict],   # Source documents
            'usage': Dict            # Token usage stats
        }
    """
```

**Caracter√≠sticas:**
- ‚úÖ Prompt engineering con sistema y contexto
- ‚úÖ Par√°metros configurables (temperature, top_p)
- ‚úÖ Citaci√≥n de fuentes
- ‚úÖ Estad√≠sticas de tokens

### 3. `valid_prompt()`

**Ubicaci√≥n:** `bedrock_utils.py`

Valida y categoriza prompts del usuario.

```python
def valid_prompt(user_prompt: str) -> Dict[str, Any]:
    """
    Validates and categorizes user prompts.
    
    Returns:
        {
            'is_valid': bool,         # Prompt is acceptable
            'category': str,          # vacation, benefits, etc.
            'confidence': float,      # 0.0 - 1.0
            'entities': Dict,         # Extracted numbers, dates
            'recommendation': str,    # process, reject, clarify
            'reason': str             # Explanation
        }
    """
```

**Categor√≠as detectadas:**
- `vacation` - Vacaciones, d√≠as libres
- `benefits` - Beneficios, seguros
- `salary` - Salario, compensaci√≥n
- `contract` - Contratos, renovaci√≥n
- `attendance` - Asistencia, horarios
- `general` - Consultas generales

---

## üéõÔ∏è Par√°metros del Modelo

Ver documentaci√≥n completa en: `temperature_top_p_explanation.md`

### Temperature

Controla la aleatoriedad de las respuestas:

| Valor | Comportamiento | Uso |
|-------|----------------|-----|
| 0.0 - 0.3 | Determinista, preciso | Pol√≠ticas, c√°lculos |
| 0.4 - 0.7 | Balanceado | Conversacional |
| 0.8 - 1.0 | Creativo, variado | Brainstorming |

### Top_p

Controla la diversidad del vocabulario:

| Valor | Comportamiento | Uso |
|-------|----------------|-----|
| 0.1 - 0.5 | Restrictivo | Respuestas t√©cnicas |
| 0.6 - 0.9 | Balanceado | General |
| 0.9 - 1.0 | Diverso | Conversacional |

### Configuraci√≥n Recomendada para DocSmart

```python
# Para consultas de pol√≠ticas HR
OPTIMAL_CONFIG = {
    'temperature': 0.3,  # Precisi√≥n factual
    'top_p': 0.9,        # Lenguaje natural
    'max_tokens': 1000   # Respuestas completas
}
```

---

## üß™ Pruebas y Validaci√≥n

### Test de Conexi√≥n AWS

```bash
python test_aws_credentials.py
```

**Output esperado:**
```
‚úì STS credentials valid
‚úì S3 bucket accessible
‚úì Bedrock models available
‚úì Knowledge Base accessible
```

### Test de Funciones

```bash
# Ejecutar tests de bedrock_utils
python -c "from bedrock_utils import valid_prompt; print(valid_prompt('¬øCu√°ntos d√≠as de vacaciones?'))"
```

### Test de RAG Pipeline

```python
from bedrock_utils import rag_pipeline

result = rag_pipeline(
    user_query="¬øCu√°ntos d√≠as de vacaciones tengo si llevo 1 a√±o?",
    temperature=0.3
)

assert result['validation']['is_valid'] == True
assert result['validation']['category'] == 'vacation'
assert result['retrieval']['count'] > 0
assert len(result['generation']['response']) > 0
print("‚úì All tests passed")
```

---

## üì∏ Capturas de Pantalla

Ver gu√≠a completa en: `screenshots/SCREENSHOT_GUIDE.md`

### Checklist de Capturas Requeridas

#### Creaci√≥n de Infraestructura Base (Stack 1)

- [ ] `terraform_apply_stack1_output.png` - Output completo de `terraform apply` Stack 1
- [ ] `aws_console_vpc.png` - VPC creada en AWS Console
- [ ] `aws_console_aurora_cluster.png` - Aurora cluster en RDS Console
- [ ] `aws_console_s3_bucket.png` - S3 bucket creado

#### Implementaci√≥n de Base de Conocimientos (Stack 2)

- [ ] `terraform_apply_stack2_output.png` - Output de `terraform apply` Stack 2
- [ ] `aws_console_knowledge_base.png` - Knowledge Base en Bedrock Console
- [ ] `aws_console_data_source.png` - Data Source configurada

#### Sincronizaci√≥n de Datos

- [ ] `s3_documents_uploaded.png` - Documentos en S3 bucket
- [ ] `knowledge_base_sync_started.png` - Ingestion job iniciado
- [ ] `knowledge_base_sync_complete.png` - Ingestion job completado
- [ ] `aurora_query_editor_verification.png` - Verificaci√≥n de datos en Aurora

#### Integraci√≥n de Python con Bedrock

- [ ] `bedrock_utils_query_knowledge_base.png` - C√≥digo de `query_knowledge_base()`
- [ ] `bedrock_utils_generate_response.png` - C√≥digo de `generate_response()`
- [ ] `bedrock_utils_valid_prompt.png` - C√≥digo de `valid_prompt()`

#### Par√°metros del Modelo

- [ ] `model_parameters_code.png` - C√≥digo mostrando temperature y top_p
- [ ] `model_parameters_explanation_doc.png` - Secci√≥n del documento de explicaci√≥n

#### Aplicaci√≥n de Chat Completa

- [ ] `streamlit_app_interface.png` - Interfaz completa de Streamlit
- [ ] `chat_example_vacation_query.png` - Consulta sobre vacaciones
- [ ] `chat_example_benefits_query.png` - Consulta sobre beneficios
- [ ] `chat_example_sources_cited.png` - Respuesta con fuentes citadas

---

## üîß Soluci√≥n de Problemas

### Problema: Terraform Apply Falla

**Error:** `Error creating RDS Cluster: InvalidParameterValue`

**Soluci√≥n:**
```bash
# Verificar password cumple requisitos (min 8 caracteres)
# Verificar regi√≥n soporta Aurora Serverless v2
# Verificar quotas de cuenta AWS
```

### Problema: Conexi√≥n a Aurora Falla

**Error:** `could not connect to server: Connection timed out`

**Soluci√≥n:**
```bash
# 1. Verificar Security Group permite puerto 5432
# 2. Verificar subnet group tiene subnets privadas
# 3. Usar Query Editor en AWS Console en su lugar
```

### Problema: Knowledge Base Sync Falla

**Error:** `Ingestion job failed with status: FAILED`

**Soluci√≥n:**
```bash
# 1. Verificar documentos en S3 tienen formatos soportados
# 2. Verificar IAM role tiene permisos de S3 y Secrets Manager
# 3. Verificar Aurora cluster est√° disponible
# 4. Revisar CloudWatch Logs para detalles
```

### Problema: Bedrock Model Not Found

**Error:** `ValidationException: The provided model identifier is invalid`

**Soluci√≥n:**
```bash
# 1. Habilitar model access en AWS Console > Bedrock > Model Access
# 2. Verificar modelo disponible en tu regi√≥n:
aws bedrock list-foundation-models --region us-east-1 | grep claude-3-5-sonnet

# 3. Usar modelo correcto en config.py
LLM_MODEL_ID = "anthropic.claude-3-5-sonnet-20240620-v1:0"
```

### Problema: Empty Results from Knowledge Base

**Error:** `No documents found` o `count: 0`

**Soluci√≥n:**
```bash
# 1. Verificar sincronizaci√≥n completada
aws bedrock-agent list-ingestion-jobs --knowledge-base-id ABCDEFGHIJ

# 2. Reducir score_threshold
query_knowledge_base(query, score_threshold=0.1)

# 3. Verificar datos en Aurora
psql -h $DB_ENDPOINT -U $DB_USER -d $DB_NAME -c "SELECT COUNT(*) FROM bedrock_integration.bedrock_kb;"
```

---

## ü§ù Contribuciones

Este proyecto fue desarrollado como parte del **AWS AI Engineer Nanodegree Program** de Udacity.

### Autor

- **Estudiante:** [Tu Nombre]
- **Programa:** AWS AI Engineer Nanodegree
- **Fecha:** Diciembre 2025

### Agradecimientos

- **Udacity** por el programa educativo
- **Amazon Web Services** por Bedrock y servicios cloud
- **Anthropic** por Claude 3.5 Sonnet

---

## üìÑ Licencia

Este proyecto est√° licenciado bajo la Licencia MIT. Ver archivo `LICENSE` para m√°s detalles.

---

## üìö Recursos Adicionales

### Documentaci√≥n Oficial

- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)
- [Aurora PostgreSQL with pgvector](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.VectorDB.html)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [Claude 3.5 Sonnet Documentation](https://docs.anthropic.com/claude/docs)

### Tutoriales y Gu√≠as

- [Building RAG Systems with Bedrock](https://aws.amazon.com/blogs/machine-learning/building-rag-systems/)
- [Terraform Best Practices](https://www.terraform-best-practices.com/)
- [pgvector for PostgreSQL](https://github.com/pgvector/pgvector)

---

## üìû Contacto y Soporte

Para preguntas sobre este proyecto:

- **Email:** [tu-email@example.com]
- **GitHub Issues:** [Link al repositorio]
- **Udacity Workspace:** [Link al workspace]

---

**üéì Este proyecto fue desarrollado como requisito para el AWS AI Engineer Nanodegree Program de Udacity.**

**√öltima actualizaci√≥n:** Diciembre 2025
